## S3
- store objects in buckets
- buckets need to have global unique name
- buckets are defined at region level
- global service but buckets are in regions
- Objects have a key > key is the full path
- Key > prefix + object name -> basically directories
- max size is 5TB
- if any file uploaded is more than 5 gb > upload in parts
- Objects: metadata, tags, version ID

### Security
1. User based - IAM policies
2. Resource based - bucket policies >bucket level rules > JSON based policies
                  - Object access control list
                   - Bucket ACL
Encrypt the objects using encryption keys

- Static website hosting can be done on S3
- Can version files on bucket level > easy to rollback changes basically

**Replication**
-copying is asynchronous
- to replicate existing objects - S3 object repl
- only new objects after enabling replication can be replicated

**Storage classes**
1. Standard - freq access, high availability, high cost, high durability (11 9's)
2. Intelligent tiering - moves data between access tiers according to storage. Use case - unpredictable access patterns
3. Standard IA - infrequently accessed data with rapid access when needed; retrieval cost is additional; use case -disaster recovery
4. One zone IA - infrequently accessed data in single zone
5. Glacier instant retrieval: Archive data requiring occasional access with millisecond retrieval. Use Case: Archived data that requires immediate retrieval, like medical records or media assets.
6. Glacier flexible retrieval: Archive data with occasional access and variable retrieval speeds. Use Case: Cost-sensitive data archiving with occasional access needs.
7. GLacier deep archive: lowest cost storage for long term archival.

**Choose Standard for high-availability apps and Intelligent-Tiering for unknown patterns.**
Use Glacier or Deep Archive for long-term archival at the lowest cost.
